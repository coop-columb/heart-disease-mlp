{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction System Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the Heart Disease Prediction system, from data preprocessing to model training and making predictions using the API.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Data Exploration](#data-exploration)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Using the API](#using-the-api)\n",
    "7. [Batch Processing](#batch-processing)\n",
    "8. [Caching for Performance](#caching)\n",
    "9. [System Architecture](#system-architecture)\n",
    "10. [Next Steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation <a id=\"setup\"></a>\n",
    "\n",
    "First, let's set up our environment and install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install -q pandas numpy scikit-learn tensorflow matplotlib seaborn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up notebook display\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure\n",
    "\n",
    "Let's examine the project structure to understand how the system is organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project root\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# List key directories\n",
    "for dir_name in [\"data\", \"models\", \"src\", \"api\", \"config\", \"docs\", \"reports\"]:\n",
    "    path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\n{dir_name}/ directory contents:\")\n",
    "        print(\"\\n\".join(f\"  - {f}\" for f in os.listdir(path) if not f.startswith(\".\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration <a id=\"data-exploration\"></a>\n",
    "\n",
    "Let's explore the heart disease dataset to understand its features and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    processed_data_path = os.path.join(\n",
    "        PROJECT_ROOT, \"data/processed/processed_data.npz\"\n",
    "    )\n",
    "    data = np.load(processed_data_path)\n",
    "    X = data[\"X\"]\n",
    "    y = data[\"y\"]\n",
    "    feature_names = data[\"feature_names\"]\n",
    "\n",
    "    # Convert to DataFrame for easier exploration\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"target\"] = y\n",
    "    print(\n",
    "        f\"Loaded processed data with {df.shape[0]} samples and {df.shape[1]} features (including target)\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Let's load the raw data instead.\")\n",
    "    raw_data_path = os.path.join(PROJECT_ROOT, \"data/raw/heart_disease_combined.csv\")\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    print(f\"Loaded raw data with {df.shape[0]} samples and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "for col, count in missing_values.items():\n",
    "    if count > 0:\n",
    "        print(f\"  - {col}: {count} missing values\")\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"  No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=\"target\", data=df)\n",
    "plt.title(\"Heart Disease Distribution\")\n",
    "plt.xlabel(\"Heart Disease (0=No, 1=Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "target_counts = df[\"target\"].value_counts()\n",
    "for i, count in enumerate(target_counts):\n",
    "    plt.text(i, count + 10, f\"{count} ({count/len(df):.1%})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relationships between numeric features and the target\n",
    "numeric_features = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "if \"target\" in numeric_features:\n",
    "    numeric_features.remove(\"target\")\n",
    "\n",
    "# Plot histograms for numeric features by target class\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(numeric_features) // 2 + (len(numeric_features) % 2),\n",
    "    ncols=2,\n",
    "    figsize=(16, 4 * len(numeric_features) // 2),\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    sns.histplot(data=df, x=feature, hue=\"target\", kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {feature} by Heart Disease\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for i in range(len(numeric_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=mask)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering <a id=\"feature-engineering\"></a>\n",
    "\n",
    "Let's explore the feature engineering process used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the feature engineering module\n",
    "from src.features.feature_engineering import (\n",
    "    create_feature_interactions,\n",
    "    create_medical_risk_score,\n",
    ")\n",
    "from src.utils import load_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"Loaded configuration:\")\n",
    "print(json.dumps(config.get(\"preprocessing\", {}), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature interactions\n",
    "df_with_interactions = df.copy()\n",
    "interactions_data = create_feature_interactions(df.drop(\"target\", axis=1))\n",
    "for col in interactions_data.columns:\n",
    "    df_with_interactions[col] = interactions_data[col]\n",
    "\n",
    "print(f\"Original dataset: {df.shape[1]} features\")\n",
    "print(f\"With interactions: {df_with_interactions.shape[1]} features\")\n",
    "print(\"\\nNew interaction features:\")\n",
    "interaction_cols = [col for col in df_with_interactions.columns if \"_x_\" in col]\n",
    "for col in interaction_cols[:5]:  # Show first 5 interaction features\n",
    "    print(f\"  - {col}\")\n",
    "if len(interaction_cols) > 5:\n",
    "    print(f\"  ... and {len(interaction_cols) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate medical risk score calculation\n",
    "risk_score_df = df.copy()\n",
    "risk_score = create_medical_risk_score(df.drop(\"target\", axis=1))\n",
    "risk_score_df[\"medical_risk_score\"] = risk_score\n",
    "\n",
    "# Visualize the medical risk score distribution by heart disease status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    data=risk_score_df,\n",
    "    x=\"medical_risk_score\",\n",
    "    hue=\"target\",\n",
    "    kde=True,\n",
    "    element=\"step\",\n",
    "    bins=20,\n",
    ")\n",
    "plt.title(\"Medical Risk Score Distribution by Heart Disease\")\n",
    "plt.xlabel(\"Medical Risk Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training <a id=\"model-training\"></a>\n",
    "\n",
    "Let's explore how models are trained in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model training functions\n",
    "from src.models.mlp_model import build_sklearn_mlp, build_keras_mlp\n",
    "\n",
    "# Load model configurations\n",
    "print(\"MLP Model configurations:\")\n",
    "print(json.dumps(config.get(\"model\", {}), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for demonstration\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(\"target\", axis=1).values  # Use just the raw dataset for simplicity\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a small scikit-learn MLP for demonstration\n",
    "sklearn_params = {\n",
    "    \"hidden_layer_sizes\": (50, 25),\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"alpha\": 0.0001,\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"max_iter\": 200,  # Small value for demo\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Build model\n",
    "sklearn_mlp = build_sklearn_mlp(params=sklearn_params)\n",
    "\n",
    "# Train model\n",
    "sklearn_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "sklearn_score = sklearn_mlp.score(X_test_scaled, y_test)\n",
    "print(f\"scikit-learn MLP Accuracy: {sklearn_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a small Keras MLP for demonstration\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Make it reproducible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "keras_mlp = build_keras_mlp(\n",
    "    input_dim,\n",
    "    architecture=[\n",
    "        {\"units\": 64, \"activation\": \"relu\", \"dropout\": 0.2, \"l2_regularization\": 0.01},\n",
    "        {\"units\": 32, \"activation\": \"relu\", \"dropout\": 0.2, \"l2_regularization\": 0.01},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "keras_mlp.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train model (with a small number of epochs for demonstration)\n",
    "history = keras_mlp.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=30,  # Small value for demo\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "_, keras_score = keras_mlp.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Keras MLP Accuracy: {keras_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Keras training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation <a id=\"model-evaluation\"></a>\n",
    "\n",
    "Let's evaluate the model performance more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate scikit-learn model\n",
    "from src.models.mlp_model import combine_predictions\n",
    "\n",
    "# Get predictions\n",
    "sklearn_preds = sklearn_mlp.predict(X_test_scaled)\n",
    "sklearn_probs = sklearn_mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Get Keras predictions\n",
    "keras_probs = keras_mlp.predict(X_test_scaled).flatten()\n",
    "keras_preds = (keras_probs >= 0.5).astype(int)\n",
    "\n",
    "# Combine predictions\n",
    "ensemble_probs = combine_predictions(sklearn_probs, keras_probs, method=\"mean\")\n",
    "ensemble_preds = (ensemble_probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "print(\"scikit-learn MLP Classification Report:\")\n",
    "print(classification_report(y_test, sklearn_preds))\n",
    "\n",
    "print(\"\\nKeras MLP Classification Report:\")\n",
    "print(classification_report(y_test, keras_preds))\n",
    "\n",
    "print(\"\\nEnsemble Classification Report:\")\n",
    "print(classification_report(y_test, ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, sklearn_preds, \"scikit-learn MLP Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, keras_preds, \"Keras MLP Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, ensemble_preds, \"Ensemble Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# scikit-learn MLP\n",
    "fpr_sklearn, tpr_sklearn, _ = roc_curve(y_test, sklearn_probs)\n",
    "roc_auc_sklearn = auc(fpr_sklearn, tpr_sklearn)\n",
    "plt.plot(\n",
    "    fpr_sklearn, tpr_sklearn, label=f\"scikit-learn MLP (AUC = {roc_auc_sklearn:.3f})\"\n",
    ")\n",
    "\n",
    "# Keras MLP\n",
    "fpr_keras, tpr_keras, _ = roc_curve(y_test, keras_probs)\n",
    "roc_auc_keras = auc(fpr_keras, tpr_keras)\n",
    "plt.plot(fpr_keras, tpr_keras, label=f\"Keras MLP (AUC = {roc_auc_keras:.3f})\")\n",
    "\n",
    "# Ensemble\n",
    "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, ensemble_probs)\n",
    "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
    "plt.plot(fpr_ensemble, tpr_ensemble, label=f\"Ensemble (AUC = {roc_auc_ensemble:.3f})\")\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the API <a id=\"using-the-api\"></a>\n",
    "\n",
    "Now let's learn how to interact with the Heart Disease Prediction API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API base URL - adjust this for your environment\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Check if API is running\n",
    "try:\n",
    "    response = requests.get(f\"{API_BASE_URL}/health\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"API is running and healthy!\")\n",
    "        print(f\"Response: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"API returned status code {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Could not connect to the API. Make sure it's running using:\")\n",
    "    print(\"  ./scripts/run_api.sh\")\n",
    "    print(\"\\nFor this tutorial, we'll proceed with simulated API responses.\")\n",
    "    API_RUNNING = False\n",
    "else:\n",
    "    API_RUNNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about available models\n",
    "if API_RUNNING:\n",
    "    response = requests.get(f\"{API_BASE_URL}/models/info\")\n",
    "    if response.status_code == 200:\n",
    "        models_info = response.json()\n",
    "        print(\"Available models:\")\n",
    "        print(json.dumps(models_info, indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "else:\n",
    "    # Simulated response\n",
    "    print(\"Simulated model information:\")\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"models_available\": {\"sklearn_mlp\": True, \"keras_mlp\": True},\n",
    "                \"ensemble_available\": True,\n",
    "                \"preprocessor_available\": True,\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example patient data\n",
    "sample_patient = {\n",
    "    \"age\": 61,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 3,\n",
    "    \"trestbps\": 140,\n",
    "    \"chol\": 240,\n",
    "    \"fbs\": 1,\n",
    "    \"restecg\": 1,\n",
    "    \"thalach\": 150,\n",
    "    \"exang\": 1,\n",
    "    \"oldpeak\": 2.4,\n",
    "    \"slope\": 2,\n",
    "    \"ca\": 1,\n",
    "    \"thal\": 3,\n",
    "}\n",
    "\n",
    "print(\"Sample patient data:\")\n",
    "print(json.dumps(sample_patient, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using the API\n",
    "def make_prediction(patient_data, model=None):\n",
    "    \"\"\"Make a prediction using the Heart Disease Prediction API.\"\"\"\n",
    "    if API_RUNNING:\n",
    "        url = f\"{API_BASE_URL}/predict\"\n",
    "        if model:\n",
    "            url += f\"?model={model}\"\n",
    "\n",
    "        response = requests.post(url, json=patient_data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    else:\n",
    "        # Simulated response\n",
    "        print(\"Simulating API prediction...\")\n",
    "        return {\n",
    "            \"prediction\": 1,\n",
    "            \"probability\": 0.9876,\n",
    "            \"risk_level\": \"HIGH\",\n",
    "            \"interpretation\": \"HIGH RISK PREDICTION: 98.8% probability of heart disease\\n\\nKey risk factors identified:\\n- Advanced age (over 55)\\n- Male over 45\\n- Chest pain at rest\\n- Exercise-induced angina\\n\\nRecommendations:\\n- Consult with a cardiologist\\n- Consider stress test or other cardiac evaluations\\n- Review medication and lifestyle modifications\",\n",
    "            \"model_used\": \"ensemble\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Make prediction using ensemble (default)\n",
    "prediction_result = make_prediction(sample_patient)\n",
    "print(\"\\nPrediction Result:\")\n",
    "print(json.dumps(prediction_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try predictions with different models\n",
    "models = [\"sklearn\", \"keras\", \"ensemble\"]\n",
    "\n",
    "results = {}\n",
    "for model in models:\n",
    "    result = make_prediction(sample_patient, model)\n",
    "    if result:\n",
    "        results[model] = result\n",
    "\n",
    "# Compare results\n",
    "for model, result in results.items():\n",
    "    print(f\"\\n{model.upper()} Model:\")\n",
    "    print(f\"  Prediction: {result['prediction']}\")\n",
    "    print(f\"  Probability: {result['probability']:.4f}\")\n",
    "    print(f\"  Risk Level: {result['risk_level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing <a id=\"batch-processing\"></a>\n",
    "\n",
    "Now let's see how to perform batch predictions for multiple patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of sample patients\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_sample_patients(count=10):\n",
    "    \"\"\"Generate a batch of random patient data for testing.\"\"\"\n",
    "    patients = []\n",
    "\n",
    "    for _ in range(count):\n",
    "        patient = {\n",
    "            \"age\": random.randint(30, 80),\n",
    "            \"sex\": random.randint(0, 1),\n",
    "            \"cp\": random.randint(0, 3),\n",
    "            \"trestbps\": random.randint(100, 180),\n",
    "            \"chol\": random.randint(150, 350),\n",
    "            \"fbs\": random.randint(0, 1),\n",
    "            \"restecg\": random.randint(0, 2),\n",
    "            \"thalach\": random.randint(100, 200),\n",
    "            \"exang\": random.randint(0, 1),\n",
    "            \"oldpeak\": round(random.uniform(0, 4), 1),\n",
    "            \"slope\": random.randint(0, 2),\n",
    "            \"ca\": random.randint(0, 3),\n",
    "            \"thal\": random.randint(1, 3),\n",
    "        }\n",
    "        patients.append(patient)\n",
    "\n",
    "    return patients\n",
    "\n",
    "\n",
    "# Generate sample batch\n",
    "sample_batch = generate_sample_patients(5)\n",
    "print(f\"Generated a batch of {len(sample_batch)} patients\")\n",
    "print(\"\\nSample patient from batch:\")\n",
    "print(json.dumps(sample_batch[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batch prediction\n",
    "def make_batch_prediction(patients, model=None):\n",
    "    \"\"\"Make batch predictions using the Heart Disease Prediction API.\"\"\"\n",
    "    if API_RUNNING:\n",
    "        url = f\"{API_BASE_URL}/predict/batch\"\n",
    "        if model:\n",
    "            url += f\"?model={model}\"\n",
    "\n",
    "        response = requests.post(url, json=patients)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    else:\n",
    "        # Simulated response\n",
    "        print(\"Simulating API batch prediction...\")\n",
    "        predictions = []\n",
    "        for _ in range(len(patients)):\n",
    "            prob = random.uniform(0, 1)\n",
    "            pred = 1 if prob >= 0.5 else 0\n",
    "            risk = \"HIGH\" if prob >= 0.6 else (\"MODERATE\" if prob >= 0.3 else \"LOW\")\n",
    "            predictions.append(\n",
    "                {\n",
    "                    \"prediction\": pred,\n",
    "                    \"probability\": prob,\n",
    "                    \"risk_level\": risk,\n",
    "                    \"model_used\": \"ensemble\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"performance_metrics\": {\n",
    "                \"total_patients\": len(patients),\n",
    "                \"processing_time_seconds\": 0.125,\n",
    "                \"throughput_patients_per_second\": len(patients) / 0.125,\n",
    "                \"num_chunks\": 1,\n",
    "                \"chunk_size\": 50,\n",
    "                \"num_workers\": 4,\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Make batch prediction\n",
    "batch_result = make_batch_prediction(sample_batch)\n",
    "\n",
    "# Display results summary\n",
    "if batch_result:\n",
    "    print(\"\\nBatch Prediction Results:\")\n",
    "\n",
    "    # Summarize predictions\n",
    "    predictions = batch_result[\"predictions\"]\n",
    "    positive_count = sum(1 for p in predictions if p[\"prediction\"] == 1)\n",
    "    negative_count = len(predictions) - positive_count\n",
    "\n",
    "    print(f\"Total patients: {len(predictions)}\")\n",
    "    print(\n",
    "        f\"Positive predictions: {positive_count} ({positive_count/len(predictions):.1%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Negative predictions: {negative_count} ({negative_count/len(predictions):.1%})\"\n",
    "    )\n",
    "\n",
    "    # Risk level breakdown\n",
    "    risk_levels = {\"LOW\": 0, \"MODERATE\": 0, \"HIGH\": 0}\n",
    "    for p in predictions:\n",
    "        risk_levels[p[\"risk_level\"]] += 1\n",
    "\n",
    "    print(\"\\nRisk level breakdown:\")\n",
    "    for level, count in risk_levels.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {level}: {count} ({count/len(predictions):.1%})\")\n",
    "\n",
    "    # Performance metrics\n",
    "    if \"performance_metrics\" in batch_result:\n",
    "        metrics = batch_result[\"performance_metrics\"]\n",
    "        print(\"\\nPerformance metrics:\")\n",
    "        print(f\"  Processing time: {metrics['processing_time_seconds']:.3f} seconds\")\n",
    "        print(\n",
    "            f\"  Throughput: {metrics['throughput_patients_per_second']:.2f} patients/second\"\n",
    "        )\n",
    "        print(f\"  Chunks: {metrics['num_chunks']} (size: {metrics['chunk_size']})\")\n",
    "        print(f\"  Workers: {metrics['num_workers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch configuration\n",
    "if API_RUNNING:\n",
    "    response = requests.get(f\"{API_BASE_URL}/batch/config\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        batch_config = response.json()\n",
    "        print(\"Current Batch Configuration:\")\n",
    "        print(json.dumps(batch_config, indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "else:\n",
    "    # Simulated response\n",
    "    print(\"Simulated batch configuration:\")\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\"batch_size\": 50, \"max_workers\": 4, \"performance_logging\": True}, indent=2\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch configuration (if API is running)\n",
    "if API_RUNNING:\n",
    "    new_config = {\"batch_size\": 100, \"max_workers\": 8, \"performance_logging\": True}\n",
    "\n",
    "    response = requests.post(f\"{API_BASE_URL}/batch/config\", json=new_config)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        updated_config = response.json()\n",
    "        print(\"Updated Batch Configuration:\")\n",
    "        print(json.dumps(updated_config, indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching for Performance <a id=\"caching\"></a>\n",
    "\n",
    "Let's explore the caching system that improves prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cache statistics\n",
    "if API_RUNNING:\n",
    "    response = requests.get(f\"{API_BASE_URL}/cache/stats\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        cache_stats = response.json()\n",
    "        print(\"Current Cache Statistics:\")\n",
    "        print(json.dumps(cache_stats, indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "else:\n",
    "    # Simulated response\n",
    "    print(\"Simulated cache statistics:\")\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"enabled\": True,\n",
    "                \"max_size\": 1000,\n",
    "                \"ttl_seconds\": 3600,\n",
    "                \"entries\": 25,\n",
    "                \"hits\": 120,\n",
    "                \"misses\": 30,\n",
    "                \"hit_rate\": 0.8,\n",
    "                \"evictions\": 0,\n",
    "                \"created_at\": \"2025-03-31T15:00:00.000000\",\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cache performance improvement\n",
    "if API_RUNNING:\n",
    "    # Make first prediction (cache miss)\n",
    "    print(\"Making first prediction (should be a cache miss)...\")\n",
    "    start_time = time.time()\n",
    "    result1 = make_prediction(sample_patient)\n",
    "    first_time = time.time() - start_time\n",
    "    print(f\"Time taken: {first_time:.4f} seconds\")\n",
    "\n",
    "    # Make second prediction (cache hit)\n",
    "    print(\"\\nMaking second prediction with same data (should be a cache hit)...\")\n",
    "    start_time = time.time()\n",
    "    result2 = make_prediction(sample_patient)\n",
    "    second_time = time.time() - start_time\n",
    "    print(f\"Time taken: {second_time:.4f} seconds\")\n",
    "\n",
    "    # Calculate speedup\n",
    "    if first_time > 0 and second_time > 0:\n",
    "        speedup = first_time / second_time\n",
    "        print(f\"\\nCache speedup: {speedup:.2f}x faster\")\n",
    "\n",
    "    # Check updated cache statistics\n",
    "    response = requests.get(f\"{API_BASE_URL}/cache/stats\")\n",
    "    if response.status_code == 200:\n",
    "        cache_stats = response.json()\n",
    "        print(\"\\nUpdated Cache Statistics:\")\n",
    "        print(json.dumps(cache_stats, indent=2))\n",
    "else:\n",
    "    # Simulated cache performance\n",
    "    print(\"Simulating cache performance...\")\n",
    "    print(\"First prediction (cache miss): 0.1500 seconds\")\n",
    "    print(\"Second prediction (cache hit): 0.0100 seconds\")\n",
    "    print(\"Cache speedup: 15.00x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update cache configuration (if API is running)\n",
    "if API_RUNNING:\n",
    "    new_cache_config = {\n",
    "        \"enabled\": True,\n",
    "        \"max_size\": 2000,\n",
    "        \"ttl\": 7200,  # 2 hours in seconds\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{API_BASE_URL}/cache/config\", json=new_cache_config)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        updated_config = response.json()\n",
    "        print(\"Updated Cache Configuration:\")\n",
    "        print(json.dumps(updated_config, indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch processing with cache\n",
    "# First clear the cache to start fresh\n",
    "if API_RUNNING:\n",
    "    response = requests.post(f\"{API_BASE_URL}/cache/clear\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Cache cleared successfully!\")\n",
    "\n",
    "    # Generate a larger batch for testing\n",
    "    large_batch = generate_sample_patients(50)\n",
    "\n",
    "    # First batch prediction (cache misses)\n",
    "    print(\"\\nMaking first batch prediction (should be cache misses)...\")\n",
    "    start_time = time.time()\n",
    "    first_batch_result = make_batch_prediction(large_batch)\n",
    "    first_batch_time = time.time() - start_time\n",
    "\n",
    "    if \"performance_metrics\" in first_batch_result:\n",
    "        metrics1 = first_batch_result[\"performance_metrics\"]\n",
    "        print(f\"Time taken: {metrics1['processing_time_seconds']:.4f} seconds\")\n",
    "        print(\n",
    "            f\"Throughput: {metrics1['throughput_patients_per_second']:.2f} patients/second\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Time taken: {first_batch_time:.4f} seconds\")\n",
    "\n",
    "    # Second batch prediction with same data (cache hits)\n",
    "    print(\"\\nMaking second batch prediction with same data (should be cache hits)...\")\n",
    "    start_time = time.time()\n",
    "    second_batch_result = make_batch_prediction(large_batch)\n",
    "    second_batch_time = time.time() - start_time\n",
    "\n",
    "    if \"performance_metrics\" in second_batch_result:\n",
    "        metrics2 = second_batch_result[\"performance_metrics\"]\n",
    "        print(f\"Time taken: {metrics2['processing_time_seconds']:.4f} seconds\")\n",
    "        print(\n",
    "            f\"Throughput: {metrics2['throughput_patients_per_second']:.2f} patients/second\"\n",
    "        )\n",
    "\n",
    "        # Calculate speedup\n",
    "        if (\n",
    "            metrics1[\"processing_time_seconds\"] > 0\n",
    "            and metrics2[\"processing_time_seconds\"] > 0\n",
    "        ):\n",
    "            speedup = (\n",
    "                metrics1[\"processing_time_seconds\"]\n",
    "                / metrics2[\"processing_time_seconds\"]\n",
    "            )\n",
    "            throughput_increase = (\n",
    "                metrics2[\"throughput_patients_per_second\"]\n",
    "                / metrics1[\"throughput_patients_per_second\"]\n",
    "            )\n",
    "            print(f\"\\nCache speedup: {speedup:.2f}x faster\")\n",
    "            print(f\"Throughput increase: {throughput_increase:.2f}x\")\n",
    "    else:\n",
    "        print(f\"Time taken: {second_batch_time:.4f} seconds\")\n",
    "        if first_batch_time > 0 and second_batch_time > 0:\n",
    "            speedup = first_batch_time / second_batch_time\n",
    "            print(f\"\\nCache speedup: {speedup:.2f}x faster\")\n",
    "\n",
    "    # Check cache statistics after batch operations\n",
    "    response = requests.get(f\"{API_BASE_URL}/cache/stats\")\n",
    "    if response.status_code == 200:\n",
    "        cache_stats = response.json()\n",
    "        print(\"\\nCache Statistics After Batch Operations:\")\n",
    "        print(json.dumps(cache_stats, indent=2))\n",
    "else:\n",
    "    # Simulated batch cache performance\n",
    "    print(\"Simulating batch processing with caching...\")\n",
    "    print(\n",
    "        \"First batch (50 patients, cache misses): 0.5000 seconds (100.00 patients/second)\"\n",
    "    )\n",
    "    print(\n",
    "        \"Second batch (50 patients, cache hits): 0.0050 seconds (10000.00 patients/second)\"\n",
    "    )\n",
    "    print(\"Cache speedup: 100.00x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture <a id=\"system-architecture\"></a>\n",
    "\n",
    "Let's examine the system architecture of the Heart Disease Prediction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.path as path\n",
    "import numpy as np\n",
    "\n",
    "# Create a simplified architecture diagram\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Define component colors\n",
    "data_color = \"#8dd3c7\"\n",
    "model_color = \"#fb8072\"\n",
    "api_color = \"#80b1d3\"\n",
    "client_color = \"#bebada\"\n",
    "utils_color = \"#fdb462\"\n",
    "\n",
    "# Define arrow properties\n",
    "arrow_props = dict(\n",
    "    arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.1\", color=\"gray\", linewidth=1.5\n",
    ")\n",
    "\n",
    "# Create background boxes\n",
    "# Data Layer\n",
    "data_layer = patches.Rectangle(\n",
    "    (0.1, 0.7),\n",
    "    0.35,\n",
    "    0.25,\n",
    "    fill=True,\n",
    "    alpha=0.3,\n",
    "    color=data_color,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.add_patch(data_layer)\n",
    "ax.text(0.275, 0.9, \"Data Layer\", ha=\"center\", fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "# Model Layer\n",
    "model_layer = patches.Rectangle(\n",
    "    (0.1, 0.4),\n",
    "    0.35,\n",
    "    0.25,\n",
    "    fill=True,\n",
    "    alpha=0.3,\n",
    "    color=model_color,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.add_patch(model_layer)\n",
    "ax.text(0.275, 0.6, \"Model Layer\", ha=\"center\", fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "# API Layer\n",
    "api_layer = patches.Rectangle(\n",
    "    (0.5, 0.4),\n",
    "    0.35,\n",
    "    0.55,\n",
    "    fill=True,\n",
    "    alpha=0.3,\n",
    "    color=api_color,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.add_patch(api_layer)\n",
    "ax.text(0.675, 0.9, \"API Layer\", ha=\"center\", fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "# Client Layer\n",
    "client_layer = patches.Rectangle(\n",
    "    (0.5, 0.1),\n",
    "    0.35,\n",
    "    0.25,\n",
    "    fill=True,\n",
    "    alpha=0.3,\n",
    "    color=client_color,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.add_patch(client_layer)\n",
    "ax.text(0.675, 0.3, \"Client Layer\", ha=\"center\", fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "# Utils (Shared)\n",
    "utils_layer = patches.Rectangle(\n",
    "    (0.1, 0.1),\n",
    "    0.35,\n",
    "    0.25,\n",
    "    fill=True,\n",
    "    alpha=0.3,\n",
    "    color=utils_color,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.add_patch(utils_layer)\n",
    "ax.text(0.275, 0.3, \"Utilities\", ha=\"center\", fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "# Add components\n",
    "# Data Layer Components\n",
    "ax.text(0.15, 0.85, \"- raw_data\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.82, \"- preprocessor\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.79, \"- train/val/test splits\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.76, \"- feature engineering\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# Model Layer Components\n",
    "ax.text(0.15, 0.55, \"- scikit-learn MLP\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.52, \"- Keras MLP\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.49, \"- Ensemble combiner\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.46, \"- Prediction cache\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.43, \"- Evaluation metrics\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# API Layer Components\n",
    "ax.text(0.55, 0.85, \"- FastAPI app\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.82, \"- Endpoints:\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.79, \"* /predict\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.76, \"* /predict/batch\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.73, \"* /batch/config\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.70, \"* /cache/stats\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.67, \"* /cache/config\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.64, \"* /cache/clear\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.61, \"* /models/info\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.57, 0.58, \"* /health\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.55, \"- Request validation\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.52, \"- Error handling\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.49, \"- Parallel processing\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.46, \"- Thread pooling\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.43, \"- Performance logging\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# Client Layer Components\n",
    "ax.text(0.55, 0.25, \"- Web UI\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.22, \"- CLI client\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.19, \"- Batch client\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.16, \"- Integration examples\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.55, 0.13, \"- Test scripts\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# Utils Components\n",
    "ax.text(0.15, 0.25, \"- Configuration loader\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.22, \"- Logging setup\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.19, \"- Path management\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.16, \"- Error handling\", fontsize=10, transform=ax.transAxes)\n",
    "ax.text(0.15, 0.13, \"- Visualization helpers\", fontsize=10, transform=ax.transAxes)\n",
    "\n",
    "# Add arrows for data flow\n",
    "# Data Layer -> Model Layer\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0.275, 0.65),\n",
    "    xytext=(0.275, 0.7),\n",
    "    arrowprops=arrow_props,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "\n",
    "# Model Layer -> API Layer\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0.5, 0.6),\n",
    "    xytext=(0.45, 0.525),\n",
    "    arrowprops=arrow_props,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "\n",
    "# API Layer -> Client Layer\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0.675, 0.35),\n",
    "    xytext=(0.675, 0.4),\n",
    "    arrowprops=arrow_props,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "\n",
    "# Utils -> All Layers\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0.275, 0.4),\n",
    "    xytext=(0.275, 0.35),\n",
    "    arrowprops=arrow_props,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0.5, 0.25),\n",
    "    xytext=(0.45, 0.25),\n",
    "    arrowprops=arrow_props,\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add title\n",
    "plt.suptitle(\"Heart Disease Prediction System Architecture\", fontsize=18, y=0.98)\n",
    "\n",
    "# Add legend for color coding\n",
    "legend_elements = [\n",
    "    patches.Patch(facecolor=data_color, alpha=0.3, label=\"Data Processing\"),\n",
    "    patches.Patch(facecolor=model_color, alpha=0.3, label=\"Model Components\"),\n",
    "    patches.Patch(facecolor=api_color, alpha=0.3, label=\"API Services\"),\n",
    "    patches.Patch(facecolor=client_color, alpha=0.3, label=\"Client Applications\"),\n",
    "    patches.Patch(facecolor=utils_color, alpha=0.3, label=\"Utility Functions\"),\n",
    "]\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.02),\n",
    "    ncol=5,\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Highlights\n",
    "\n",
    "The Heart Disease Prediction system employs a layered architecture that separates concerns and promotes maintainability:\n",
    "\n",
    "1. **Data Layer**:\n",
    "   - Responsible for data acquisition, preprocessing, and feature engineering\n",
    "   - Manages train/validation/test splits and data transformations\n",
    "   - Stores processed data and preprocessing artifacts\n",
    "\n",
    "2. **Model Layer**:\n",
    "   - Implements multiple machine learning models (scikit-learn MLP, Keras MLP)\n",
    "   - Provides ensemble prediction capability\n",
    "   - Includes model evaluation metrics and interpretation\n",
    "   - Implements prediction caching for improved performance\n",
    "\n",
    "3. **API Layer**:\n",
    "   - Exposes models through REST endpoints using FastAPI\n",
    "   - Provides single and batch prediction capabilities\n",
    "   - Implements parallel processing for batch predictions\n",
    "   - Includes configuration endpoints for caching and batch settings\n",
    "   - Implements comprehensive error handling and validation\n",
    "\n",
    "4. **Client Layer**:\n",
    "   - Multiple interfaces for interacting with the system (Web UI, CLI, etc.)\n",
    "   - Provides examples for integration with other systems\n",
    "   - Includes testing scripts for validation\n",
    "\n",
    "5. **Utilities (Shared)**:\n",
    "   - Common functionality used across layers\n",
    "   - Configuration management\n",
    "   - Logging and path handling\n",
    "   - Visualization helpers\n",
    "\n",
    "### Key Performance Features\n",
    "\n",
    "The system implements several optimizations for high performance:\n",
    "\n",
    "1. **Prediction Caching**:\n",
    "   - LRU (Least Recently Used) caching for repeated predictions\n",
    "   - Configurable TTL (Time-To-Live) for cache entries\n",
    "   - Hash-based cache keys for efficient lookups\n",
    "   - Statistics tracking for monitoring cache performance\n",
    "\n",
    "2. **Parallel Processing**:\n",
    "   - Chunking of large batches for efficient processing\n",
    "   - Thread pool for parallel execution of prediction chunks\n",
    "   - Configurable chunk size and worker count for tuning\n",
    "\n",
    "3. **Model Selection**:\n",
    "   - Multiple model options with different performance characteristics\n",
    "   - Ensemble approach for improved accuracy\n",
    "   - Fallback mechanisms for handling model unavailability\n",
    "\n",
    "This architecture ensures the system is scalable, maintainable, and performs well under various workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps <a id=\"next-steps\"></a>\n",
    "\n",
    "Based on the project roadmap, here are the next steps for enhancing the Heart Disease Prediction system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Enhancements\n",
    "\n",
    "1. **API Authentication**:\n",
    "   - Implement API key authentication\n",
    "   - Add user management capabilities\n",
    "   - Implement role-based access control\n",
    "\n",
    "2. **Deployment Improvements**:\n",
    "   - Implement backup and recovery procedures\n",
    "   - Create environment-specific configuration for dev/staging/prod\n",
    "   - Enhance Docker deployment with multi-stage builds\n",
    "\n",
    "### Performance Optimizations\n",
    "\n",
    "1. **Distributed Caching**:\n",
    "   - Implement Redis-based caching for multi-instance deployments\n",
    "   - Add cache persistence for resilience\n",
    "\n",
    "2. **Model Optimization**:\n",
    "   - Implement model quantization for faster inference\n",
    "   - Explore TensorFlow Lite for edge deployment\n",
    "\n",
    "### Feature Enhancements\n",
    "\n",
    "1. **Advanced Models**:\n",
    "   - Add gradient boosting model (XGBoost, LightGBM)\n",
    "   - Implement Bayesian neural networks for uncertainty quantification\n",
    "   - Add explainability with SHAP/LIME integration\n",
    "\n",
    "2. **User Experience**:\n",
    "   - Create downloadable report format for predictions\n",
    "   - Enhance visualization components\n",
    "   - Implement batch upload interface\n",
    "\n",
    "### Monitoring and Telemetry\n",
    "\n",
    "1. **Performance Monitoring**:\n",
    "   - Add dashboard for monitoring cache performance and API throughput\n",
    "   - Implement telemetry for tracking prediction errors\n",
    "   - Create centralized logging system\n",
    "\n",
    "2. **Model Drift Detection**:\n",
    "   - Implement automated monitoring for model drift\n",
    "   - Create alerts for performance degradation\n",
    "   - Add scheduled retraining capability\n",
    "\n",
    "By implementing these enhancements, the Heart Disease Prediction system will become even more robust, secure, and user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has provided a comprehensive overview of the Heart Disease Prediction system, including:\n",
    "\n",
    "- Data exploration and feature engineering\n",
    "- Model training and evaluation\n",
    "- Using the API for predictions\n",
    "- Implementing batch processing for efficiency\n",
    "- Leveraging caching for improved performance\n",
    "- Understanding the system architecture\n",
    "\n",
    "The system demonstrates good practices in machine learning engineering, with a focus on:\n",
    "\n",
    "- Model performance and evaluation\n",
    "- Scalable and efficient API design\n",
    "- Performance optimization through caching and parallel processing\n",
    "- Comprehensive error handling and validation\n",
    "- Clear system architecture with separation of concerns\n",
    "\n",
    "By following this tutorial, you should now have a good understanding of how to use and extend the Heart Disease Prediction system for your own applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
