{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction System Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the Heart Disease Prediction MLP system. It covers:\n",
    "\n",
    "1. Data loading and exploration\n",
    "2. Model training and evaluation\n",
    "3. Making predictions\n",
    "4. Using the API\n",
    "\n",
    "> **Note:** This notebook requires specific dependencies. Please ensure you've installed all requirements with `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import requests\n",
    "\n",
    "# Set the style for plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Check if tabulate is available for better table formatting\n",
    "try:\n",
    "    import tabulate\n",
    "    print(\"Tabulate dependency is available!\")\n",
    "except ImportError:\n",
    "    print(\"Warning: Tabulate dependency is missing. Install with 'pip install tabulate'\")\n",
    "    \n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "    \n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root directory: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We'll first load the heart disease dataset. We'll try to load the processed data. If that's not available, we'll generate synthetic data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load processed data\n",
    "try:\n",
    "    # First try to load processed data splits\n",
    "    data_path = os.path.join(PROJECT_ROOT, 'data/processed/processed_data.npz')\n",
    "    print(f\"Attempting to load data from: {data_path}\")\n",
    "    \n",
    "    data = np.load(data_path)\n",
    "    print(f\"Available arrays in the data file: {list(data.keys())}\")\n",
    "    \n",
    "    # Load training data\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    \n",
    "    # If available, load feature names from metadata\n",
    "    try:\n",
    "        metadata_path = os.path.join(PROJECT_ROOT, 'data/processed/processing_metadata.txt')\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        feature_names = metadata.get('feature_names', [])\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        # Default feature names if metadata not available\n",
    "        feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                         'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "    \n",
    "    # Create DataFrame for exploring training data\n",
    "    if len(feature_names) == X_train.shape[1]:\n",
    "        df_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "        df_train['target'] = y_train\n",
    "        print(f\"Successfully loaded processed data with {df_train.shape[0]} samples and {df_train.shape[1]} features (including target)\")\n",
    "    else:\n",
    "        print(f\"Warning: Feature names count ({len(feature_names)}) doesn't match data columns ({X_train.shape[1]})\")\n",
    "        df_train = pd.DataFrame(X_train)\n",
    "        df_train['target'] = y_train\n",
    "        print(f\"Created DataFrame without feature names, shape: {df_train.shape}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Creating synthetic data for demonstration.\")\n",
    "    # Create synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                     'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "    \n",
    "    # Generate random but somewhat realistic features\n",
    "    X = np.zeros((n_samples, len(feature_names)))\n",
    "    X[:, 0] = np.random.normal(55, 10, n_samples)  # age\n",
    "    X[:, 1] = np.random.binomial(1, 0.7, n_samples)  # sex\n",
    "    X[:, 2] = np.random.randint(0, 4, n_samples)  # cp\n",
    "    X[:, 3] = np.random.normal(130, 15, n_samples)  # trestbps\n",
    "    X[:, 4] = np.random.normal(220, 40, n_samples)  # chol\n",
    "    X[:, 5] = np.random.binomial(1, 0.2, n_samples)  # fbs\n",
    "    X[:, 6] = np.random.randint(0, 3, n_samples)  # restecg\n",
    "    X[:, 7] = np.random.normal(150, 20, n_samples)  # thalach\n",
    "    X[:, 8] = np.random.binomial(1, 0.3, n_samples)  # exang\n",
    "    X[:, 9] = np.random.exponential(1, n_samples)  # oldpeak\n",
    "    X[:, 10] = np.random.randint(0, 3, n_samples)  # slope\n",
    "    X[:, 11] = np.random.randint(0, 4, n_samples)  # ca\n",
    "    X[:, 12] = np.random.choice([3, 6, 7], n_samples)  # thal\n",
    "    \n",
    "    # Generate target variable (some simple rules to create correlations)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[X[:, 0] > 60] += 0.2  # older age increases risk\n",
    "    y[X[:, 1] == 1] += 0.2  # male increases risk\n",
    "    y[X[:, 2] > 1] += 0.3  # chest pain type > 1 increases risk\n",
    "    y[X[:, 4] > 240] += 0.2  # high cholesterol increases risk\n",
    "    y[X[:, 7] < 140] += 0.2  # low max heart rate increases risk\n",
    "    y[X[:, 8] == 1] += 0.3  # exercise induced angina increases risk\n",
    "    y[X[:, 9] > 1.5] += 0.3  # high ST depression increases risk\n",
    "    y[X[:, 11] > 0] += 0.2 * X[:, 11]  # more colored vessels increase risk\n",
    "    \n",
    "    # Convert to binary target (with some randomness)\n",
    "    y = (y + np.random.normal(0, 0.1, n_samples) > 0.5).astype(int)\n",
    "    \n",
    "    # Create train/test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    df_train['target'] = y_train\n",
    "    \n",
    "    print(f\"Created synthetic dataset with {df_train.shape[0]} training samples and {df_train.shape[1]} features (including target)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    print(\"Creating minimal synthetic dataset as fallback\")\n",
    "    \n",
    "    # Create very simple synthetic data as fallback\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "    X_train = np.random.rand(n_samples, len(feature_names))\n",
    "    y_train = np.random.randint(0, 2, size=n_samples)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    df_train['target'] = y_train\n",
    "    print(f\"Created minimal synthetic dataset with {df_train.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic statistics\n",
    "print(\"\\nBasic statistics of the dataset:\")\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    stats = df_train.describe().T\n",
    "    stats['missing'] = df_train.isnull().sum()\n",
    "    print(tabulate(stats, headers='keys', tablefmt='grid'))\n",
    "except Exception as e:\n",
    "    print(f\"Could not use tabulate due to: {str(e)}\")\n",
    "    display(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='target', data=df_train)\n",
    "plt.title('Distribution of Heart Disease')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['No (0)', 'Yes (1)'])\n",
    "\n",
    "# Add count labels on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_train.corr()\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, mask=mask)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Pre-trained Models\n",
    "\n",
    "The Heart Disease Prediction system comes with pre-trained models. Let's load and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure loading time for performance evaluation\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Try to import our model predictor\n",
    "    from src.models.predict_model import HeartDiseasePredictor\n",
    "    \n",
    "    # Create a predictor instance\n",
    "    model_predictor = HeartDiseasePredictor(model_dir=\"models\")\n",
    "    \n",
    "    # Check which models are available\n",
    "    print(\"Available models:\")\n",
    "    print(f\"  - Scikit-learn MLP model: {'Yes' if model_predictor.has_sklearn_model else 'No'}\")\n",
    "    print(f\"  - Keras MLP model: {'Yes' if model_predictor.has_keras_model else 'No'}\")\n",
    "    print(f\"  - Ensemble model: {'Yes' if model_predictor.has_ensemble_model else 'No'}\")\n",
    "    print(f\"  - Preprocessor available: {'Yes' if model_predictor.preprocessor is not None else 'No'}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nModel loading time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # If we couldn't load models, explain how to train new ones\n",
    "    if not model_predictor.has_sklearn_model and not model_predictor.has_keras_model:\n",
    "        print(\"\\nNo pre-trained models found. You can train new models using the scripts/train_models.sh script.\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\nError importing modules: {str(e)}\")\n",
    "    print(\"Make sure you have the project installed properly and Python path is set correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading models: {str(e)}\")\n",
    "    print(\"Will continue with directly using scikit-learn instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with Pre-trained Models\n",
    "\n",
    "Let's take a sample from our data and make predictions using the loaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get a sample patient from our data\n",
    "    sample_patient = df_train.iloc[10].copy()\n",
    "    sample_target = sample_patient.pop('target')\n",
    "    sample_dict = sample_patient.to_dict()\n",
    "    \n",
    "    print(\"Sample patient data:\")\n",
    "    for k, v in sample_dict.items():\n",
    "        print(f\"  {k}: {v:.2f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    print(f\"Actual target: {sample_target}\")\n",
    "    \n",
    "    # Make predictions if we have the model predictor loaded\n",
    "    if 'model_predictor' in locals():\n",
    "        # Performance measurement\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Predict with all available models\n",
    "        prediction_result = model_predictor.predict(\n",
    "            sample_dict,\n",
    "            return_probabilities=True,\n",
    "            return_interpretation=True\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\nPrediction time: {end_time - start_time:.4f} seconds\")\n",
    "        \n",
    "        # Display prediction results\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        \n",
    "        # Check which model was used\n",
    "        if \"model_used\" in prediction_result:\n",
    "            print(f\"Model used: {prediction_result['model_used']}\")\n",
    "            \n",
    "        # Display predictions from different models if available\n",
    "        if \"sklearn_predictions\" in prediction_result:\n",
    "            print(f\"Scikit-learn prediction: {prediction_result['sklearn_predictions'][0]}\")\n",
    "            if \"sklearn_probabilities\" in prediction_result:\n",
    "                print(f\"Scikit-learn probability: {prediction_result['sklearn_probabilities'][0]:.4f}\")\n",
    "                \n",
    "        if \"keras_predictions\" in prediction_result:\n",
    "            print(f\"Keras prediction: {prediction_result['keras_predictions'][0]}\")\n",
    "            if \"keras_probabilities\" in prediction_result:\n",
    "                print(f\"Keras probability: {prediction_result['keras_probabilities'][0]:.4f}\")\n",
    "                \n",
    "        if \"ensemble_predictions\" in prediction_result:\n",
    "            print(f\"Ensemble prediction: {prediction_result['ensemble_predictions'][0]}\")\n",
    "            if \"ensemble_probabilities\" in prediction_result:\n",
    "                print(f\"Ensemble probability: {prediction_result['ensemble_probabilities'][0]:.4f}\")\n",
    "                \n",
    "        # Show interpretation if available\n",
    "        if \"interpretation\" in prediction_result:\n",
    "            print(f\"\\nInterpretation:\\n{prediction_result['interpretation']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {str(e)}\")\n",
    "    print(\"Will demonstrate with a simpler model in the next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model Training (Fallback)\n",
    "\n",
    "If the pre-trained models aren't available, let's train a simple model using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if we couldn't load the pre-trained models\n",
    "if 'model_predictor' not in locals() or not (model_predictor.has_sklearn_model or model_predictor.has_keras_model):\n",
    "    print(\"Training a simple scikit-learn model...\")\n",
    "    \n",
    "    try:\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare data\n",
    "        X = df_train.drop('target', axis=1)\n",
    "        y = df_train['target']\n",
    "        \n",
    "        # Create a train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create a pipeline with preprocessing and model\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(16, 8), max_iter=1000, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        score = pipeline.score(X_test, y_test)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate time\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Model training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Accuracy on test set: {score:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot ROC curve if probabilities are available\n",
    "        if y_proba is not None:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.show()\n",
    "        \n",
    "        # Make a simple prediction with our trained model\n",
    "        sample_patient = X_test.iloc[0]\n",
    "        sample_features = sample_patient.values.reshape(1, -1)\n",
    "        \n",
    "        prediction_time_start = time.time()\n",
    "        prediction = pipeline.predict(sample_features)[0]\n",
    "        probability = pipeline.predict_proba(sample_features)[0, 1] if hasattr(pipeline, 'predict_proba') else None\n",
    "        prediction_time_end = time.time()\n",
    "        \n",
    "        print(f\"\\nSample Prediction:\")\n",
    "        print(f\"Prediction time: {prediction_time_end - prediction_time_start:.4f} seconds\")\n",
    "        print(f\"Predicted class: {prediction}\")\n",
    "        if probability is not None:\n",
    "            print(f\"Probability: {probability:.4f}\")\n",
    "        print(f\"Actual class: {y_test.iloc[0]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the API\n",
    "\n",
    "The Heart Disease Prediction system provides a REST API for making predictions. Let's demonstrate how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prediction_api(patient_data, api_url=\"http://localhost:8000/predict\"):\n",
    "    \"\"\"\n",
    "    Call the prediction API with the given patient data.\n",
    "    \n",
    "    Args:\n",
    "        patient_data (dict): Dictionary containing patient features\n",
    "        api_url (str): URL of the prediction API endpoint\n",
    "        \n",
    "    Returns:\n",
    "        dict: API response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Make the API request\n",
    "        response = requests.post(api_url, json=patient_data, timeout=5)\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"API request successful in {elapsed_time:.4f} seconds\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"API request failed with status code {response.status_code} in {elapsed_time:.4f} seconds\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"\\nConnection error: Could not connect to the API.\")\n",
    "        print(\"Make sure the API server is running using 'python run_api.py' or 'bash scripts/run_api.sh'\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError calling API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a sample patient for API request\n",
    "sample_api_patient = {\n",
    "    \"age\": 61,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 3,\n",
    "    \"trestbps\": 140,\n",
    "    \"chol\": 240,\n",
    "    \"fbs\": 1,\n",
    "    \"restecg\": 1,\n",
    "    \"thalach\": 150,\n",
    "    \"exang\": 1,\n",
    "    \"oldpeak\": 2.4,\n",
    "    \"slope\": 2,\n",
    "    \"ca\": 1,\n",
    "    \"thal\": 3\n",
    "}\n",
    "\n",
    "# Display the patient data\n",
    "print(\"Sample patient data for API:\")\n",
    "for k, v in sample_api_patient.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Call the API\n",
    "print(\"\\nCalling prediction API...\")\n",
    "try:\n",
    "    result = call_prediction_api(sample_api_patient)\n",
    "    \n",
    "    # Display the API response\n",
    "    if result:\n",
    "        print(\"\\nAPI Response:\")\n",
    "        for k, v in result.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Interpret the result\n",
    "        prediction = result.get('prediction')\n",
    "        probability = result.get('probability')\n",
    "        risk_level = result.get('risk_level')\n",
    "        model_used = result.get('model_used')\n",
    "        \n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"The model predicts {'positive' if prediction == 1 else 'negative'} for heart disease\")\n",
    "        print(f\"Probability: {probability:.2f}\")\n",
    "        print(f\"Risk level: {risk_level}\")\n",
    "        print(f\"Model used: {model_used}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing API result: {str(e)}\")\n",
    "    print(\"Note: You need to start the API server to use this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Example\n",
    "\n",
    "Let's demonstrate how to use the batch processing API for making predictions on multiple patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_batch_prediction_api(patients_data, api_url=\"http://localhost:8000/predict/batch\"):\n",
    "    \"\"\"\n",
    "    Call the batch prediction API with multiple patient data.\n",
    "    \n",
    "    Args:\n",
    "        patients_data (list): List of dictionaries containing patient features\n",
    "        api_url (str): URL of the batch prediction API endpoint\n",
    "        \n",
    "    Returns:\n",
    "        dict: API response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Make the API request\n",
    "        response = requests.post(api_url, json=patients_data, timeout=10)\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"Batch API request successful in {elapsed_time:.4f} seconds\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"Batch API request failed with status code {response.status_code} in {elapsed_time:.4f} seconds\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"\\nConnection error: Could not connect to the API.\")\n",
    "        print(\"Make sure the API server is running using 'python run_api.py' or 'bash scripts/run_api.sh'\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError calling batch API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of sample patients\n",
    "def generate_patient_batch(n_patients=5):\n",
    "    \"\"\"\n",
    "    Generate a batch of sample patients.\n",
    "    \n",
    "    Args:\n",
    "        n_patients (int): Number of patients to generate\n",
    "        \n",
    "    Returns:\n",
    "        list: List of patient dictionaries\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    batch = []\n",
    "    \n",
    "    for i in range(n_patients):\n",
    "        patient = {\n",
    "            \"age\": int(np.random.normal(55, 10)),\n",
    "            \"sex\": int(np.random.binomial(1, 0.7)),\n",
    "            \"cp\": int(np.random.randint(0, 4)),\n",
    "            \"trestbps\": int(np.random.normal(130, 15)),\n",
    "            \"chol\": int(np.random.normal(220, 40)),\n",
    "            \"fbs\": int(np.random.binomial(1, 0.2)),\n",
    "            \"restecg\": int(np.random.randint(0, 3)),\n",
    "            \"thalach\": int(np.random.normal(150, 20)),\n",
    "            \"exang\": int(np.random.binomial(1, 0.3)),\n",
    "            \"oldpeak\": float(np.round(np.random.exponential(1), 1)),\n",
    "            \"slope\": int(np.random.randint(0, 3)),\n",
    "            \"ca\": int(np.random.randint(0, 4)),\n",
    "            \"thal\": int(np.random.choice([3, 6, 7]))\n",
    "        }\n",
    "        batch.append(patient)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Generate 10 sample patients\n",
    "patient_batch = generate_patient_batch(10)\n",
    "\n",
    "# Display the first patient in the batch\n",
    "print(f\"Generated a batch of {len(patient_batch)} patients for batch processing\")\n",
    "print(\"\\nFirst patient in batch:\")\n",
    "for k, v in patient_batch[0].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Call the batch API\n",
    "print(\"\\nCalling batch prediction API...\")\n",
    "try:\n",
    "    batch_result = call_batch_prediction_api(patient_batch)\n",
    "    \n",
    "    # Display the batch API response summary\n",
    "    if batch_result:\n",
    "        predictions = batch_result.get('predictions', [])\n",
    "        performance_metrics = batch_result.get('performance_metrics', {})\n",
    "        \n",
    "        print(f\"\\nReceived {len(predictions)} predictions from batch API\")\n",
    "        \n",
    "        # Show performance metrics if available\n",
    "        if performance_metrics:\n",
    "            print(\"\\nPerformance Metrics:\")\n",
    "            for k, v in performance_metrics.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Create a summary table of predictions\n",
    "        print(\"\\nPrediction Summary:\")\n",
    "        summary_df = pd.DataFrame(predictions)\n",
    "        print(summary_df[['prediction', 'probability', 'risk_level', 'model_used']].head(10))\n",
    "        \n",
    "        # Plot distribution of predictions\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        summary_df['risk_level'].value_counts().plot(kind='bar')\n",
    "        plt.title('Distribution of Risk Levels in Batch Predictions')\n",
    "        plt.xlabel('Risk Level')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error processing batch API result: {str(e)}\")\n",
    "    print(\"Note: You need to start the API server to use this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Environment-Specific Configuration\n",
    "\n",
    "The Heart Disease Prediction system supports environment-specific configuration. Let's explore how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.utils import load_config\n",
    "    \n",
    "    # Load the configuration\n",
    "    config = load_config()\n",
    "    \n",
    "    # Display configuration sections\n",
    "    print(\"Configuration Sections:\")\n",
    "    for section in config.keys():\n",
    "        print(f\"  - {section}\")\n",
    "    \n",
    "    # Show API configuration\n",
    "    if 'api' in config:\n",
    "        print(\"\\nAPI Configuration:\")\n",
    "        for key, value in config['api'].items():\n",
    "            if not isinstance(value, dict):\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show model configuration\n",
    "    if 'models' in config:\n",
    "        print(\"\\nModel Configuration:\")\n",
    "        for key, value in config['models'].items():\n",
    "            if not isinstance(value, dict):\n",
    "                print(f\"  {key}: {value}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing config module: {str(e)}\")\n",
    "    print(\"Will show example config structure instead\")\n",
    "    \n",
    "    # Example config structure\n",
    "    example_config = {\n",
    "        \"api\": {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 8000,\n",
    "            \"batch_size\": 50,\n",
    "            \"max_workers\": 4,\n",
    "            \"caching\": {\n",
    "                \"enabled\": True,\n",
    "                \"max_size\": 1000,\n",
    "                \"ttl\": 3600\n",
    "            }\n",
    "        },\n",
    "        \"models\": {\n",
    "            \"model_dir\": \"models\",\n",
    "            \"default_model\": \"ensemble\"\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"processed_dir\": \"data/processed\",\n",
    "            \"raw_dir\": \"data/raw\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExample Configuration Structure:\")\n",
    "    print(json.dumps(example_config, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "1. Loading and exploring heart disease prediction data\n",
    "2. Using pre-trained MLP models\n",
    "3. Making predictions using the API\n",
    "4. Batch processing capabilities\n",
    "5. Environment-specific configuration\n",
    "\n",
    "Next steps you might want to explore:\n",
    "- Experiment with different model hyperparameters\n",
    "- Try feature engineering techniques\n",
    "- Explore model interpretability in more depth\n",
    "- Implement a custom web interface for the API\n",
    "\n",
    "For more details, see the project documentation in the `/docs` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}